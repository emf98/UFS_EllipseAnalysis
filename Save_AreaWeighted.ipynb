{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab55179f-0697-4a37-b046-2ed5cb452b9e",
   "metadata": {},
   "source": [
    "## Obtain area-weighted UFS data. \n",
    "\n",
    "Temp, GPH, and maybe PV?\n",
    "\n",
    "Last modified 12/1/2025 to change order of indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c09cd945-a179-4276-84b0-a23b646a6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports for relevant packages \n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0333a4-e96e-4883-a6ec-e7c78a29b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(target):\n",
    "    weights=np.cos(np.deg2rad(target.latitude))\n",
    "    weight = target.weighted(weights).mean(dim=\"latitude\").mean(dim=\"longitude\") #average over the full area\n",
    "    #print(weight.shape)\n",
    "    return weight;\n",
    "\n",
    "#modified to work with this data. \n",
    "def daily_anomaly(target):\n",
    "    #daily mean across years AND forecasts\n",
    "    dailymean = np.nanmean(target, axis=(0, 1))\n",
    "    anom = target - dailymean[None, None, :]\n",
    "    return anom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c072a5-a06f-4731-8eac-58ba9e28e259",
   "metadata": {},
   "source": [
    "Start with are weighted GPH at 100hPa over the North Atlantic.\n",
    "\n",
    "lat=slice(80,30),lon = slice(260,350)\n",
    "\n",
    "I need all year/forecast combos available to calculate the anomaly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f8407d-f379-4ca5-8dc6-760b5209143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists to loop through for naming strings per season. \n",
    "years = [y for y in range(2011,2018,1)]\n",
    "month_date = [(11,1),(11,15),(12,1),(12,15),(1,1),(1,15),(2,1),(2,15)]\n",
    "#indicate list of potential prototypes\n",
    "prototype = [\"Prototype5\",\"Prototype6\",\"Prototype7\",\"Prototype8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b296fc-eee0-45e5-b9c0-c3b281ace94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype5 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n",
      "Prototype6 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n",
      "Prototype7 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n",
      "Prototype8 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n"
     ]
    }
   ],
   "source": [
    "geopotential = np.empty((7,8,4,36))\n",
    "#conduct loop to calculate and save GPH metric.\n",
    "for p in range(len(prototype)):\n",
    "    dummy = np.empty((7,8,36))\n",
    "    print(f'{prototype[p]} running ...')\n",
    "    for i in range(len(years)):\n",
    "        for j in range(len(month_date)):\n",
    "            if month_date[j][0] == 11 or month_date[j][0] == 12:\n",
    "                #designate file-specific date\n",
    "                date_str = f\"{years[i]}{month_date[j][0]:02d}{month_date[j][1]:02d}\"\n",
    "                #open file\n",
    "                gfile = xr.open_dataset( f\"/network/rit/lab/wulab/forecast/s2s/ufs/{prototype[p]}/gh/gh_plevs_{date_str}.nc\")\n",
    "                g_files = gfile[\"gh\"]\n",
    "                g_data = g_files.loc[dict(latitude=slice(80,30),longitude=slice(260,350),lev=100)]\n",
    "                #create weight\n",
    "                g_weighted = weights(g_data)\n",
    "                #append to dummy array\n",
    "                dummy[i,j,:] = g_weighted\n",
    "\n",
    "            else:\n",
    "                #designate file-specific date\n",
    "                date_str = f\"{years[i]+1}{month_date[j][0]:02d}{month_date[j][1]:02d}\"\n",
    "                #open file\n",
    "                gfile = xr.open_dataset( f\"/network/rit/lab/wulab/forecast/s2s/ufs/{prototype[p]}/gh/gh_plevs_{date_str}.nc\")\n",
    "                g_files = gfile[\"gh\"]\n",
    "                g_data = g_files.loc[dict(latitude=slice(80,30),longitude=slice(260,350),lev=100)]\n",
    "                #create weight\n",
    "                g_weighted = weights(g_data)\n",
    "                #append to dummy array\n",
    "                dummy[i,j,:] = g_weighted\n",
    "    print(\"Finish collecting by Prototype, calculating anomaly...\")\n",
    "    #calculate day/year anomaly\n",
    "    geopotential[:,:,p,:] = daily_anomaly(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8463fc09-84ca-4981-8243-d0cf76e12af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send out GPH anomalies\n",
    "pickle.dump(geopotential, open(\"./UFS_metrics/UFS_GPHanoms100.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "653952bb-507f-4618-a772-9448d2aa8a6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "tfile = xr.open_dataset( f\"/network/rit/lab/langlab_rit/efernandez/Research/Dissertation/AI_Paper/Forecast_comparison/download_ufs/Prototype8/t_1000/t_plevs_20180201.nc\")\n",
    "t_files = tfile[\"t\"]\n",
    "t_data = t_files.loc[dict(latitude=slice(75,60),longitude=slice(10,45),lev=1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806fa5b-7df9-477d-977c-42dcf1d3c953",
   "metadata": {},
   "source": [
    "European region of interest (60-75N, 10-45W). \n",
    "\n",
    "Nova Scotia (55-70N, 70-50W/290-310) \n",
    "\n",
    "SE US (32-40N, 90-105W/255-270). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e75a0a46-2724-4d48-8fc2-a8714f853d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype5 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n",
      "Prototype6 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n",
      "Prototype7 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n",
      "Prototype8 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n"
     ]
    }
   ],
   "source": [
    "eur_temp = np.empty((7,8,4,36))\n",
    "#conduct loop to calculate and save European temp\n",
    "for p in range(len(prototype)):\n",
    "    dummy = np.empty((7,8,36))\n",
    "    print(f'{prototype[p]} running ...')\n",
    "    for i in range(len(years)):\n",
    "        for j in range(len(month_date)):\n",
    "            if month_date[j][0] == 11 or month_date[j][0] == 12:\n",
    "                #designate file-specific date\n",
    "                date_str = f\"{years[i]}{month_date[j][0]:02d}{month_date[j][1]:02d}\"\n",
    "                #open file\n",
    "                tfile = xr.open_dataset( f\"/network/rit/lab/langlab_rit/efernandez/Research/Dissertation/AI_Paper/Forecast_comparison/download_ufs/{prototype[p]}/t_1000/t_plevs_{date_str}.nc\")\n",
    "                t_files = tfile[\"t\"]\n",
    "                t_data = t_files.loc[dict(latitude=slice(75,60),longitude=slice(10,45),lev=1000)]\n",
    "                #create weight\n",
    "                t_weighted = weights(t_data)\n",
    "                #append to dummy array\n",
    "                dummy[i,j,:] = t_weighted\n",
    "\n",
    "            else:\n",
    "                #designate file-specific date\n",
    "                date_str = f\"{years[i]+1}{month_date[j][0]:02d}{month_date[j][1]:02d}\"\n",
    "                #open file\n",
    "                tfile = xr.open_dataset( f\"/network/rit/lab/langlab_rit/efernandez/Research/Dissertation/AI_Paper/Forecast_comparison/download_ufs/{prototype[p]}/t_1000/t_plevs_{date_str}.nc\")\n",
    "                t_files = tfile[\"t\"]\n",
    "                t_data = t_files.loc[dict(latitude=slice(75,60),longitude=slice(10,45),lev=1000)]\n",
    "                #create weight\n",
    "                t_weighted = weights(t_data)\n",
    "                #append to dummy array\n",
    "                dummy[i,j,:] = t_weighted\n",
    "    print(\"Finish collecting by Prototype, calculating anomaly...\")\n",
    "    #calculate day/year anomaly\n",
    "    eur_temp[:,:,p,:] = daily_anomaly(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b1cd09f-8592-4374-b409-31ffa993759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype5 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n",
      "Prototype6 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n",
      "Prototype7 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n",
      "Prototype8 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n"
     ]
    }
   ],
   "source": [
    "nova_temp = np.empty((7,8,4,36))\n",
    "#conduct loop to calculate and save European temp\n",
    "for p in range(len(prototype)):\n",
    "    dummy = np.empty((7,8,36))\n",
    "    print(f'{prototype[p]} running ...')\n",
    "    for i in range(len(years)):\n",
    "        for j in range(len(month_date)):\n",
    "            if month_date[j][0] == 11 or month_date[j][0] == 12:\n",
    "                #designate file-specific date\n",
    "                date_str = f\"{years[i]}{month_date[j][0]:02d}{month_date[j][1]:02d}\"\n",
    "                #open file\n",
    "                tfile = xr.open_dataset( f\"/network/rit/lab/langlab_rit/efernandez/Research/Dissertation/AI_Paper/Forecast_comparison/download_ufs/{prototype[p]}/t_1000/t_plevs_{date_str}.nc\")\n",
    "                t_files = tfile[\"t\"]\n",
    "                t_data = t_files.loc[dict(latitude=slice(70,55),longitude=slice(290,310),lev=1000)]\n",
    "                #create weight\n",
    "                t_weighted = weights(t_data)\n",
    "                #append to dummy array\n",
    "                dummy[i,j,:] = t_weighted\n",
    "\n",
    "            else:\n",
    "                #designate file-specific date\n",
    "                date_str = f\"{years[i]+1}{month_date[j][0]:02d}{month_date[j][1]:02d}\"\n",
    "                #open file\n",
    "                tfile = xr.open_dataset( f\"/network/rit/lab/langlab_rit/efernandez/Research/Dissertation/AI_Paper/Forecast_comparison/download_ufs/{prototype[p]}/t_1000/t_plevs_{date_str}.nc\")\n",
    "                t_files = tfile[\"t\"]\n",
    "                t_data = t_files.loc[dict(latitude=slice(70,55),longitude=slice(290,310),lev=1000)]\n",
    "                #create weight\n",
    "                t_weighted = weights(t_data)\n",
    "                #append to dummy array\n",
    "                dummy[i,j,:] = t_weighted\n",
    "    print(\"Finish collecting by Prototype, calculating anomaly...\")\n",
    "    #calculate day/year anomaly\n",
    "    nova_temp[:,:,p,:] = daily_anomaly(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09d7fb66-870c-4edf-9cb7-680e6905cf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype5 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n",
      "Prototype6 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n",
      "Prototype7 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n",
      "Prototype8 running ...\n",
      "Finish collecting by Prototype, calculating anomaly...\n"
     ]
    }
   ],
   "source": [
    "seus_temp = np.empty((7,8,4,36))\n",
    "#conduct loop to calculate and save European temp\n",
    "for p in range(len(prototype)):\n",
    "    dummy = np.empty((7,8,36))\n",
    "    print(f'{prototype[p]} running ...')\n",
    "    for i in range(len(years)):\n",
    "        for j in range(len(month_date)):\n",
    "            if month_date[j][0] == 11 or month_date[j][0] == 12:\n",
    "                #designate file-specific date\n",
    "                date_str = f\"{years[i]}{month_date[j][0]:02d}{month_date[j][1]:02d}\"\n",
    "                #open file\n",
    "                tfile = xr.open_dataset( f\"/network/rit/lab/langlab_rit/efernandez/Research/Dissertation/AI_Paper/Forecast_comparison/download_ufs/{prototype[p]}/t_1000/t_plevs_{date_str}.nc\")\n",
    "                t_files = tfile[\"t\"]\n",
    "                t_data = t_files.loc[dict(latitude=slice(40,32),longitude=slice(255,270),lev=1000)]\n",
    "                #create weight\n",
    "                t_weighted = weights(t_data)\n",
    "                #append to dummy array\n",
    "                dummy[i,j,:] = t_weighted\n",
    "\n",
    "            else:\n",
    "                #designate file-specific date\n",
    "                date_str = f\"{years[i]+1}{month_date[j][0]:02d}{month_date[j][1]:02d}\"\n",
    "                #open file\n",
    "                tfile = xr.open_dataset( f\"/network/rit/lab/langlab_rit/efernandez/Research/Dissertation/AI_Paper/Forecast_comparison/download_ufs/{prototype[p]}/t_1000/t_plevs_{date_str}.nc\")\n",
    "                t_files = tfile[\"t\"]\n",
    "                t_data = t_files.loc[dict(latitude=slice(40,32),longitude=slice(255,270),lev=1000)]\n",
    "                #create weight\n",
    "                t_weighted = weights(t_data)\n",
    "                #append to dummy array\n",
    "                dummy[i,j,:] = t_weighted\n",
    "    print(\"Finish collecting by Prototype, calculating anomaly...\")\n",
    "    #calculate day/year anomaly\n",
    "    seus_temp[:,:,p,:] = daily_anomaly(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c4e1cf4-0809-4070-b8fd-4e1c76a1c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send out Temp anomalies\n",
    "pickle.dump(eur_temp, open(\"./UFS_metrics/UFS_EurTempAnoms.p\", 'wb'))\n",
    "pickle.dump(nova_temp, open(\"./UFS_metrics/UFS_CanTempAnoms.p\", 'wb'))\n",
    "pickle.dump(seus_temp, open(\"./UFS_metrics/UFS_SEUSTempAnoms.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56c0c50e-115d-4877-81e1-f71fc42ce7bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### get appropriate ERA-5 gph and temps ..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8d6a945-0922-4dfe-950f-c078c0246432",
   "metadata": {},
   "source": [
    "infile = open(\"./era5/eur_anomtemps.p\", 'rb') \n",
    "eur = pickle.load(infile)\n",
    "eur = eur.reshape((62,151))\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"./era5/nova_anomtemps.p\", 'rb') \n",
    "can = pickle.load(infile)\n",
    "can = can.reshape((62,151))\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"./era5/seus_anomtemps.p\", 'rb') \n",
    "seus = pickle.load(infile)\n",
    "seus  = seus.reshape((62,151))\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"./era5/NA_gph_weightedANOM_100.p\", 'rb') \n",
    "gph = pickle.load(infile)\n",
    "gph = gph[:,31:].reshape((62,151))\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94f5c1d9-bacf-41f2-8e38-015922089beb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#create empty arrays for ERA5 \n",
    "e = np.empty((7,8,36))\n",
    "n = np.empty((7,8,36))\n",
    "s = np.empty((7,8,36))\n",
    "g = np.empty((7,8,36))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb2b9daa-091d-419b-97c0-203ab2a6a891",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "years = [y for y in range(52,59,1)]\n",
    "index = [0,14,30,44,61,75,92,106]\n",
    "\n",
    "for i in range(len(years)):\n",
    "    year = years[i] #make sure proper index is used for the ERA-5.\n",
    "    #i is preserved to index the copy array properly. \n",
    "    for j in range(len(index)):\n",
    "        ind = index[j]\n",
    "        e[i,j,:] = eur[year,ind:ind+36]\n",
    "        n[i,j,:] = can[year,ind:ind+36]\n",
    "        s[i,j,:] = seus[year,ind:ind+36]\n",
    "        g[i,j,:] = gph[year,ind:ind+36]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30dd7d32-aaab-4f5b-8450-459c975f7244",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#export/dump the arrays into file\n",
    "pickle.dump(e, open(\"./UFS_metrics/ERA5_eurtemp.p\", 'wb'))\n",
    "pickle.dump(n, open(\"./UFS_metrics/ERA5_novatemp.p\", 'wb')) \n",
    "pickle.dump(s, open(\"./UFS_metrics/ERA5_seustemp.p\", 'wb'))\n",
    "pickle.dump(g, open(\"./UFS_metrics/ERA5_gph.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f980606-c41f-47e4-97b8-375b0bb44bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 AI Environment",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
